---
title: "Identification of distributions for risks based on the first moment and c-statistic: with application in decision modeling, sample size calculations, and value-of-information analysis"
output:
  pdf_document:
    keep_tex: yes
    includes:
      in_header: preamble.tex
  word_document: default
  html_document:
    df_print: paged
referee: yes
bibliography: references.bib
csl: sage-vancouver.csl
header: 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{R echo=FALSE, include=FALSE}
options(tinytex.clean = FALSE)
library(tidyverse)
res_path <- "./"

my_format <- function(val, n_digits=4)
{
  format(round(val,n_digits),nsmall=n_digits, big.mark=",")
}
```


## Abstract

Decision analytic modeling, sample size calculations, and value-of-information analysis for risk prediction models often require specification of the distribution of predicted risks in the target population. However, this is seldom reported in conventional model development and validation studies. On the other hand, indirect information on this distribution is often available in terms of expected value and c-statistic. Methods that can reconstruct the distribution of risks from such information can have broad applicability. Here, we show that for a wide class of probability distributions for risks, knowing the expected value and c-statistic is enough to identify the distribution. The derivation motivates numerical algorithms for mapping a given pair of expected value and c-statistic to the parameters of specified two-parameter distributions for probabilities. In a simulation study, we evaluated the numerical accuracy of such algorithms for common families of distributions for risks (Beta, Logit-normal, and Probit-normal). In a case study we show how this approach can be used to enable Monte Carlo sampling from the posterior distribution of parameters that determine the distribution of predicted risks. The accompanying **mcmapper** R package encapsulates these developments. 

## Highlights
- Most risk modeling studies report on outcome prevalence and discriminatory performance of the model (area under the receiver operating characteristic curve / c-statistic).
- Decision analytic modeling, sample size calculations, and value-of-information analysis for risk prediction models often require specification of the distribution of predicted risks.
- We show that for a wide class of probability distributions for risks, knowing the outcome prevalence and the model's c-statistic is enough to identify the distribution.
- The accompanying open-access software provides numerical algorithms for such mapping for common distributions for risk.

\newpage

## Background

In many instances, one might desire to know the distribution of individualized risks in a population of interest. Consider, for example, a decision analysis aimed at finding the optimal cut-off on predicted risks of lung cancer, to identify high-risk individuals for referral to computed tomography. A key parameter for such an analysis is the proportion of individuals classified as high-risk at a given cut-off value, a quantity that depends on the distribution of predicted risks. In addition to decision-analytic modeling, sample size calculations and recently proposed value-of-information (VoI) analysis methods for external validation of risk prediction models require specifying a distribution for predicted risks [@Riley2021CPMSampleSizeExVal; @Pavlou2021CPMSampleSizeExVal; @Sadatsafavi2023EVPICPMExVal]. 

In decision-analytic modeling, identifying a distribution based on its moments (e.g., mean and variance) or confidence intervals is a common practice. However, in risk modeling studies, the variance of, or confidence bounds around, predicted risks is seldom reported. Riley et. al. note that sometimes the report provides some clue, for example, through a histogram of predicted risks @Riley2024CPMTutorialPart3; but parameterizing the distribution of risks based on quantitative performance metrics can be a more objective approach. Fortunately, the majority of risk prediction development or validation studies report the c-statistic of the model summarizing its discriminatory power[@Collins2024]. Generally speaking, the c-statistic provides indirect information about the spread of predicted risks (models with low c-statistic have concentrated mass, while those with high c-statistic are generally more variable). Given that common distributions for modeling probabilities (e.g., Beta, or  Logit-normal) are indexed by two parameters, it is intuitive that within a given family of such distributions, knowing the expected value and c-statistic should uniquely identify the distribution. But to the best of our knowledge, no rigorous proof has hitherto been offered.

This work builds on such intuition, and proves the uniqueness of such identification. We provide open-source software that implements numerical mapping algorithms based on such identification, examine its face validity through brief simulation studies, and its utility via a case study.

## Notation and definitions

Let $F$ be the strictly monotonic cumulative distribution function (CDF) from the family of distributions of interest with support on $[0,1]$. Let $\pi$ be a random draw from this distribution and $Y|\pi \sim \mbox{Bernoulli}(\pi)$ the corresponding response variable. Let $m$ be the first moment of $F$, and $c$ its c-statistic. $c$ is the probability that a random draw from $\pi$s among 'cases' (those with $Y=1$) is larger than a random draw from $\pi$s among 'controls' (those with $Y=0$). Formally, $c:=P(\pi_2 > \pi_1 | Y_2=1, Y_1=0)$ with $(\pi_1,Y_1)$ and $(\pi_2,Y_2)$ being two pairs of predicted risks and observed responses, with $(\pi_1,Y_1) \mathrel{\perp} (\pi_2,Y_2)$ .

We define $F$ as *quantile-identifiable* if knowing any pair of quantiles fully identifies the distribution. Common two-parameter distributions for probabilities, such as Beta ($\pi\sim \mbox{Beta}(\alpha,\beta)$), Logit-normal ($\mbox{logit}(\pi) \sim \mbox{Normal}(\mu,\sigma^2)$, where $\mbox{logit}(\pi):=\mbox{log}(\pi/(1-\pi))$) and Probit-normal ($\Phi^{-1}(\pi) \sim \mbox{Normal}(\mu,\sigma^2)$ where $\Phi(x)$ is the standard Normal CDF) satisfy the strict monotonicity and quantile-identifiability. The quantile-identifiability of the Beta distribution was established by Shih @Shih2015BetaIdentifiability. For the Logit-normal and Probit-normal distributions, it is immediately deduced from the monotonic link to the Normal distribution and the quantile-identifiability of the latter.

## Lemma

Consider $F$, a family of probability distributions with support on [0,1], with the following characteristics:

-   the CDF is strictly monotonic and has no jumps;
-   the distribution is quantile-identifiable.

Then $F$ is uniquely identifiable from {$m$, $c$}.

## Proof
Details are provided in the Appendix. In brief, we establish the following system of equations for $F$:

\begin{gather*}
\begin{cases}
\int_0^1F(x;\lambda)dx = 1-m;\\
\\
\int_0^1F^2(x;\lambda)dx = 1-2cm+(2c-1)m^2,
\end{cases}
\end{gather*}
where $\lambda$ is the set (typically a pair) of parameters indexing $F$. The proof involves showing that this system has at most one solution.


## Implementation and simulation studies

We have implemented a set of numerical algorithms for finding the parameters of specified families of distributions of the above-mentioned class given a known mean and c-statistic in the accompanying [**mcmapper**](https://cran.r-project.org/package=mcmapper) R package @Sadatsafavi2024mcmapperRPackageCRAN. Details of our implementation are provided in Supplementary Material - Section 1. Supplementary Material - Section 2 provides the results of brief simulation studies that document the satisfactory performance of these algorithms across a wide range of expected value and c-statistic ($m \in [0.01,0.99]$ and $c \in [0.51,0.99]$). 


## Case study

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
library(tidyverse)
library(voipred)
library(pROC)
library(WeightedROC)
library(sqldf)
library(mcmapper)
library(bayespmtools)
library(fitdistrplus)
library(ggpubr)

set.seed(1) 

my_format <- function(val, n_digits=4)
{
  format(round(val,n_digits),nsmall=n_digits)
}

data(gusto)
gusto$kill <- (as.numeric(gusto$Killip)>1)*1
gusto$Y <- gusto$day30
data_us <- gusto[gusto$regl %in% c(1, 7, 9, 10, 11, 12, 14, 15),]
data_other <- gusto[!gusto$regl %in% c(1, 7, 9, 10, 11, 12, 14, 15),]


dev_data <- gusto[sample(nrow(gusto), size=500), ]

#model <- glm(Y ~ age + miloc + pmi + kill + pmin(sysbp,100) + lsp(pulse,50), data=dev_data, family=binomial(link="logit"))
model <- glm(Y ~ age + miloc + pmi + kill + pmin(sysbp,100) + pulse, data=dev_data, family=binomial(link="logit"))


model_equation <- function(model, proper_coeff_names=NULL, ...) { # from https://stats.stackexchange.com/questions/63600/how-to-translate-the-results-from-lm-to-an-equation
  format_args <- list(...)

  model_coeff <- model$coefficients
  if(is.null(proper_coeff_names))
  {
    coeff_names <- names(model_coeff)
  }
  else
  {
    coeff_names <- proper_coeff_names[names(model_coeff)]
  }

  format_args$x <- abs(model$coefficients)
  model_coeff_sign <- sign(model_coeff)
  model_coeff_prefix <- case_when(model_coeff_sign == -1 ~ " - ",
                                  model_coeff_sign == 1 ~ " + ",
                                  model_coeff_sign == 0 ~ " + ")
  model_eqn <- paste(#model$family$link,"(",strsplit(as.character(model$call$formula), "~")[[2]],")" # 'y'
                     "logit(P(Y=1))=",
                     paste(if_else(model_coeff[1]<0, "- ", ""),
                           do.call(format, format_args)[1],
                           paste(model_coeff_prefix[-1],
                                 do.call(format, format_args)[-1],
                                 "",
                                 coeff_names[-1],
                                 sep = "", collapse = ""),
                           sep = ""))
  return(model_eqn)
}


proper_coeff_names <- c("(Intercept)"="",
                              "age"="[age]",
                              "milocOther"="[AMI location other (vs. inferior)]",
                              "milocAnterior"="[AMI location anterior (v. inferior)]",
                              "pmiyes"="[previous AMI]",
                              "kill"="[AMI severity score)]",
                              "pmin(sysbp, 100)"="[min(blood pressure, 100)]",
                              "pulse"="[pulse rate]")


model_eq <- model_equation(model, proper_coeff_names, digits=3)


dev_data$pi <- predict(model, type="response")

n <- nrow(dev_data)
waucs <- rep(NA, 10000)
for(i in 1:10000)
{
  w <- rgamma(n,1,1)
  tp.fp <- WeightedROC(dev_data$pi, dev_data$Y, w/sum(w))
  waucs[i] <- WeightedAUC(tp.fp)
}
cstat <- WeightedAUC(WeightedROC(dev_data$pi, dev_data$Y))
cstat_ci <- quantile(waucs, c(0.025,0.975))

# cstat_ci <- ci(Y~pi, data=dev_data, quiet=T)
#cstat_parms <- inv_moments("beta", c(cstat, ((cstat_ci[2]-cstat_ci[1])/1.96)^2))
f <- function(x) {tmp=qbeta(c(0.025,0.975),x,x/cstat-x); (tmp[2]-tmp[1])-(cstat_ci[2]-cstat_ci[1])}
res <- uniroot(f=f,interval=c(1,1000))
cstat_parms <- c(shape1=res$root, shape2=res$root/cstat-res$root)
prev_parms  <- c(shape1=sum(dev_data$Y), shape2=length(dev_data$Y)-sum(dev_data$Y))

#beta_parms <- fitdistrplus::fitdist(dev_data$pi, distr="beta", method="mme")$estimate
beta_parms <- mcmap_beta(c(mean(dev_data$Y), cstat))

```

Consider a probabilistic micro-simulation model for the evaluation of a triaging algorithm that identifies individuals at high risk of death after an acute myocardial infarction (AMI) upon their arrival at an emergency department. Individuals are assigned a mortality risk from the distribution of this risk in the population. Parameters that index this distribution are inferred from finite-sample studies. Quantifying uncertainty around their values (i.e., second-order uncertainty) is required for a probabilistic decision analysis. The report on the development of the underlying risk prediction model often contains an estimate of the expected value of risk $\hat{\mathbb{E}}(Y)$ and the c-statistic of the model for predicting $Y$. Assuming that the model is moderately calibrated in the development population (i.e., $\mathbb{E}(Y|\pi)=\pi$), the identifiability result in this work can directly be applied to recover the parameters of the distribution. 

To illustrate this, we create a risk prediction model for 30-day mortality after an AMI based on data from 500 observations from the GUSTO-I clinical trial. This setup replicates a previous case study (where more details are provided) [@Sadatsafavi2025EVSIExval]. The model equation is 

\indent{`r model_eq`}.

Out of `r nrow(dev_data)`, `r sum(dev_data$Y)` experienced the outcome ($\hat{\mathbb{E}}(Y)=`r my_format(mean(dev_data[,'Y']),3)`$). The c-statistic of the model in this sample was `r my_format(cstat,3)`, with a 95% credible interval, quantified using the Bayesian bootstrap method[@Gu2008] of `r my_format(cstat_ci[1],3)`-`r my_format(cstat_ci[2],3)`.

If the original development sample was available, one could use sample-based methods, such as Bayesian bootstrapping, to quantify uncertainty around the distribution of predicted risks[@Pasta1999]. Assuming we do not have access to the sample, we use the summary statistics above as the source of information. We parameterize the distribution of prevalence as $m\sim\mbox{Beta}(`r prev_parms[1]`,`r prev_parms[2]`)$, and that of the c-statistic as $c\sim\mbox{Beta}(`r my_format(cstat_parms[1],1)`,`r my_format(cstat_parms[2],1)`)$. The former is based on standard Beta-Binomial Bayesian modeling, and the latter is based on finding the two parameters of the Beta distribution whose mean and central 95% width match, respectively, the point estimate and 95% credible interval width as reported above.

We use the Beta distribution to characterize predicted risks, which is indexed by two parameters $\alpha$ and $\beta$. Our goal is to quantify our uncertainty around the joint distribution of $\alpha$ and $\beta$. This can be achieved by repeatedly sampling from the assigned distributions to prevalence ($m$) and c-statistic ($c$), applying our mapping algorithm, and recording the resulting $\alpha$ and $\beta$ values. We note that this sampling algorithm implements independent draws from $m$ and $c$, despite the fact that they are estimated from the same data. However, as shown in  Supplementary Material - Section 3, the posterior distributions of $m$ and $c$ are independent if such distributions are constructed via Bayesian bootstrapping in the original sample.

**Figure 1** - panel A shows the scatter plot of 10,000 pairs of sampled $\alpha$ and $\beta$ values. Panel B shows the histogram of the individualized risks from the development sample along with the density of the Beta distribution based on point estimates of prevalence and c-statistic. In addition, the 2.5th and 97.5th percentiles of the density values at each point are overlaid on the histogram.


```{r, echo=FALSE, message=FALSE, results='hide', fig.width=10, fig.height=5, fig.cap="**A**: Scatter plot of ($\\alpha$, $\\beta$) pairs. **B**: Histogram of predicted risks (bars), the density of the Beta distribution based on point estimates of prevalence and c-statistic (solid line), and central 95% boundaries of Beta densities from the Monte Carlo sample (dashed lines - note that dashed lines are not valid densities in themselves; rather, at each point they represent the 95% boundary of the density values from the Monte Carlo simulation)"}

X <- (1:100)/100

n_sim <- 1000
cols <- c('shape1','shape2')
out <- matrix(NA, nrow=n_sim, ncol=length(cols))
colnames(out) <- cols

cstats <- rbeta(n_sim, cstat_parms[1], cstat_parms[2])
prevs <- rbeta(n_sim, prev_parms[1], prev_parms[2])

dens_vals <- matrix(NA, nrow=n_sim, ncol=length(X))

for(i in 1:n_sim)
{
  out[i,] <- mcmap_beta(target=c(prevs[i], cstats[i]),optim_controls=list(upper=100))
  dens_vals[i, ] <- dbeta(X, out[i,1], out[i,2])
  cat('.')
}

Q <- apply(dens_vals, 2, quantile, c(0.025,0.975))


# 
# hist(dev_data$pi, freq=F, breaks=20,xlim=c(0, max(dev_data$pi)))
# lines(X, dbeta(X, beta_parms[1], beta_parms[2]), col="blue", lwd="2")
# lines(X, Q[1,], col="black")
# lines(X, Q[2,], col="black")

df <- data.frame(X=X, density=dbeta(X, beta_parms[1], beta_parms[2]), density_low=Q[1,], density_high=Q[2,])

plt_hist <- ggplot() +
  # 1. Histogram for dev_data$pi
  # The primary data for the x-axis comes from dev_data
  geom_histogram(
    data = dev_data,
    aes(x = pi, y = after_stat(density)), # using after_stat(density) for modern ggplot2
    bins = 20,                          # equivalent to breaks = 20
    fill = "orange",
    color = "orange",
    alpha = 0.7
  )+
  geom_line(
    data = df,
    aes(x = X, y = density),
    color = "blue",
    linewidth = 0.75 # lwd="2" is thicker; ggplot's linewidth is often in mm
  )+
  geom_line(
    data = df,
    aes(x = X, y = density_low),
    color = "black",
    linewidth = 0.5, linetype="dashed" # lwd="2" is thicker; ggplot's linewidth is often in mm
  )+
  geom_line(
    data = df,
    aes(x = X, y = density_high),
    color = "black",
    linewidth = 0.5, linetype="dashed" # lwd="2" is thicker; ggplot's linewidth is often in mm
  )+
  labs(
    title = "",
    x = "Predicted risks",
    y = "Density"
  )+
  theme_minimal()


plt_scat <- ggplot(as.data.frame(out), aes(x=shape1, y=shape2)) + geom_point() + theme_minimal() + labs(
    title = "",
    x = expression(alpha),
    y = expression(beta)
  )

ggarrange(plt_scat, plt_hist, labels = c("A", "B"), ncol=2, nrow=1)

```


## Remarks
We showed that strictly continuous distributions for risks that are quantile-identifiable can be identified by knowing their expected value and c-statistic. The identifiability motivates numerical mapping algorithms that are implemented into the publicly available **mcmapper** R package. In a case study we showed how summary statistics from a model development study can be used to enable uncertainty quantification around the parameters of the distribution of risks. In pursuing these developments we were motivated by the need for probabilistic modeling of the distribution of predicted risks in decision analysis, as well as several recent developments around sample size calculations and VoI analysis for risk prediction models[@Riley2021CPMSampleSizeExVal; @Sadatsafavi2023EVPICPMExVal]. 

While the mapping algorithms performed well in our brief simulation studies, they might be numerically unstable at extreme ranges, especially for the c-statistic. This is because the CDF can be very flat (within the floating point precision of the computing device) in extended parts of its range in such cases, making numerical integration inaccurate. However, extreme values of the c-statistic are not plausible in real-world situations. A c-statistic that is very close to 1, for example, might indicate a complete separation of cases and controls. Reciprocally, a c-statistic that is very close to 0.5 signals no variability in risks. In such situations, the appropriateness of modeling the risk as a continuous distribution might be doubtful.

In assigning a distribution type to the distribution of risks (e.g. Beta in our case study), the onus is on the investigator to decide on the appropriateness of the family of distributions. Sometimes there is auxiliary information in a report, such as the histogram of predicted risks, that the investigator can draw on, and perform sensitivity analyses as required.

Overall, given the ubiquity of reporting prevalence and c-statistic in the development and validation studies of risk prediction models, we expect this approach to play an important role in uncertainty quantification when evaluating such models. Health technology assessments, sample size calculations, and VoI analyses are immediate areas of application for this methodology. 

# References
<div id="refs"></div>

\clearpage

# Appendix: Proof of the main lemma

First, we use the standard result that for $\pi$ as a non-negative random variable, $\mathbb{E}(\pi)=1 - \int_0^1  F(x)dx$. Thus, knowing $m$ is equivalent to knowing the area under the CDF. Next, applying Bayes' rule to the distribution of $\pi$ among cases ($P(\pi|Y=1)$) and controls ($P(\pi|Y=0)$) reveals that the former has a PDF of $xf(x)/m$ and the latter $(1-x)f(x)/(1-m)$, where $f(x):=dF(x)/dx$ is the PDF of $F$. Thus we have

\begin{flalign*}
m(1-m)c&=\int_0^1\left(\int_0^x(1-y)f(y)dy\right)xf(x)dx \\ &=\int_0^1\left(\int_0^x f(y)dy\right)xf(x)dx-\int_0^1\left(\int_0^x yf(y)dy\right)xf(x)dx \\ &=\int_0^1F(x)xf(x)dx-\int_0^1g(x)G(x)dx,
\end{flalign*}

where $g(x)=xf(x)$ and $G(x)=\int_0^x g(y)dy$. Integration by parts for the first term and a change of variable for the second term result in

\begin{gather*}
m(1-m)c=\frac{1}{2}xF^2(x)|_0^1-\frac{1}{2}\int_0^1F^2(x)dx-\frac{1}{2}G^2(x)|_0^1=\frac{1}{2}-\frac{1}{2}\int_0^1F^2(x)dx-\frac{1}{2}m^2;
\end{gather*}

i.e., among the subset of $F$s with the same $m$, $c$ is monotonically related to $\int_0^1F^2(x)dx$. As such, the goal is achieved by showing that {$\int_0^1F(x)dx$, $\int_0^1F^2(x)dx$} uniquely identifies $F$. We prove this by showing that two different CDFs $F_1$ and $F_2$ from the same family and with the same $\int_0^1F(x)dx$ cannot have the same $\int_0^1F^2(x)dx$.

Given that both CDFs are anchored at (0,0) and (1,1), are strictly monotonic, and have the same area under the CDF but are not equal at all points, they must cross. However, they can only cross once, given the quantile-identifiability requirement (if they cross two or more times, any pairs of quantiles defined by the crossing points would fail to identify them uniquely).

Let $x^*$ be the unique crossing point of the two CDFs, and let $y^*=F_1(x^*)=F_2(x^*)$ be the CDF value at this point. We break $\int_0^1(F_1^2(x)-F_2^2(x))dx$ into two parts around $x^*$:

\begin{gather*}
\int_0^1 (F_1^2(x)-F_2^2(x))dx=\int_0^{x^*} \left(F_1(x)-F_2(x)\right)\left(F_1(x)+F_2(x)\right)dx+ \int_{x^*}^1 \left(F_1(x)-F_2(x)\right)\left(F_1(x)+F_2(x)\right)dx.
\end{gather*}

Without loss of generality, assume we label the $F$s such that $F_1(x) > F_2(x)$ when $x \in (0,{x^*})$. In this region, due to the CDFs strictly increasing, $0<F_1(x)+F_2(x) < F_1({x^*})+F_2({x^*}) =2y^*$. As such, replacing $F_1(x)+F_2(x)$ by the larger positive quantity $2y^*$ will increase this term. As well, in the $x\in ({x^*},1)$ region, $F_1(x)-F_2(x) < 0$, and $0<F_1({x^*})+F_2({x^*})=2y^*<F_1(x)+F_2(x)$. As such, replacing $F_1(x)+F_2(x)$ by the smaller positive quantity $2y^*$ will also increase this term. Therefore we have

\begin{gather*}
\int_0^1 (F_1^2(x)-F_2^2(x))dx < 2y^* \left(\int_0^{x^*} (F_1(x)-F_2(x))dx+\int_{x^*}^1 (F_1(x)-F_2(x))dx\right),
\end{gather*}

and the term on the right-hand side is zero because of the equality of the area under the CDFs. Therefore, $\int_0^1 (F_1^2(x)-F_2^2(x))dx < 0$, establishing the desired result.


